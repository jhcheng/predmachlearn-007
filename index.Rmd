---
title: "Practical Machine Learning : Course Project"
author: "Horace Cheng"
output: html_document
---

### Executive Summary

The human activity recognition research has traditionally focused on discriminating between different activities, i.e. to predict "which" activity was performed at a specific point in time. This project use the **Weight Lifting Exercises** dataset to investigate "how (well)" an activity was performed by the wearer. The detail information about this dataset can refer to: [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har)

The goal of this project is to predict the manner in which they did the exercise. This is the **classe** variable in the training set. After building the model and test with the test data, we have the following findings:

- **Preprocessing** dataset is necessary in building models. Initially there are *146* predictors in dataset. After preprocessing, we reduce the number of predictor to *53*.
- ```Random Forest``` method works very well in this project. The out of sample error is only *0.0024(0.24%)*. However, we still have to do some experiments for other possible candidates to pick up the best choices.
- **Cross Validation** do have a little improvement in the final model. But cross validation will make training time longer. So we need to balance between folds/repeat times and accuracy of the model.

### Preprocessing Data
```{r, warning=FALSE,message=FALSE}
# load library
library(caret)
```

1. Read the training data first:
```{r}
# read data
pml <- read.csv("pml-training.csv", stringsAsFactors = F)
data <- pml[,-(1:5)]
rm(pml)
```

2. We can see some variables should be *number* but read as *char*. So we have to change their type to number:
```{r, warning=FALSE}
# some variable is numeric but wrongly read as char
data$kurtosis_roll_belt <- as.numeric(data$kurtosis_roll_belt)
data$kurtosis_picth_belt <- as.numeric(data$kurtosis_picth_belt)
data$skewness_roll_belt <- as.numeric(data$skewness_roll_belt)
data$skewness_roll_belt.1 <- as.numeric(data$skewness_roll_belt.1)
data$max_yaw_belt <- as.numeric(data$max_yaw_belt)
data$min_yaw_belt <- as.numeric(data$min_yaw_belt)
data$kurtosis_roll_arm <- as.numeric(data$kurtosis_roll_arm)
data$kurtosis_picth_arm <- as.numeric(data$kurtosis_picth_arm)
data$kurtosis_yaw_arm <- as.numeric(data$kurtosis_yaw_arm)
data$skewness_roll_arm <- as.numeric(data$skewness_roll_arm)
data$skewness_pitch_arm <- as.numeric(data$skewness_pitch_arm)
data$skewness_yaw_arm <- as.numeric(data$skewness_yaw_arm)
data$kurtosis_roll_dumbbell <- as.numeric(data$kurtosis_roll_dumbbell)
data$kurtosis_picth_dumbbell <- as.numeric(data$kurtosis_picth_dumbbell)
data$skewness_roll_dumbbell <- as.numeric(data$skewness_roll_dumbbell)
data$skewness_pitch_dumbbell <- as.numeric(data$skewness_pitch_dumbbell)
data$max_yaw_dumbbell <- as.numeric(data$max_yaw_dumbbell)
data$min_yaw_dumbbell <- as.numeric(data$min_yaw_dumbbell)
data$kurtosis_roll_forearm <- as.numeric(data$kurtosis_roll_forearm)
data$kurtosis_picth_forearm <- as.numeric(data$kurtosis_picth_forearm)
data$skewness_roll_forearm <- as.numeric(data$skewness_roll_forearm)
data$skewness_pitch_forearm <- as.numeric(data$skewness_pitch_forearm)
data$max_yaw_forearm <- as.numeric(data$max_yaw_forearm)
data$min_yaw_forearm <- as.numeric(data$min_yaw_forearm)
```

3. Some variables with strange or meaningless values, so we remove them from our dataset:
```{r}
# remove variables with strange or meaningless values:
data <- subset(data, select = -c(kurtosis_yaw_belt, skewness_yaw_belt,
                                 amplitude_yaw_belt, kurtosis_yaw_dumbbell,
                                 skewness_yaw_dumbbell, amplitude_yaw_dumbbell,
                                 kurtosis_yaw_forearm, skewness_yaw_forearm,
                                 amplitude_yaw_forearm))
```

4. Because there is still one char variable, so we create a dummy variable:
```{r}
# set dummy variables
data$new_window <- as.factor(data$new_window)
data$classe <- as.factor(data$classe)
dummies <- dummyVars(classe ~ ., data = data)
df <- as.data.frame(predict(dummies, newdata = data))
```

5. We don't have any information about missing values, so we assume they should be 0:
```{r}
# impute NA as 0
df[is.na(df)] <- 0
```

6. To reduce the number of predictors, we remove some zero- and near zero-variance predictors:
```{r}
# Zero- and Near Zero-Variance Predictors
nzv <- nearZeroVar(df)
df <- df[, -nzv]
```


### Training the Model
After preprocessing, only 53 predictors left in our dataset. So we start training our model.

1. Split data to training set and testing set:
```{r}
#split to training set and data set
set.seed(99889)
inTrain <- createDataPartition(data$classe, p = 0.75, list=F)
trainingX <- df[inTrain,]
trainingY <- data$classe[inTrain]
testingX <- df[-inTrain,]
testingY <- data$classe[-inTrain]
```

2. Cross Validation: we use ```repeatedcv``` for repeated k-fold cross-validation(repeat 10 times and k = 10). 
```{r}
# use trainControl to control the resampling
# repeated(10 times) K(K=10) fold cross validation
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 10, classProbs = TRUE)
```

3. Start training by ```Random Forest``` model. We choose this model by following steps:
  - First, this is a classification problem with multiple classes. So the ```Multiple Logistic Regression```, ```Tree-Based method``` and ``Support Vector Machine`` are 3 possible options we can use.
  - Second, we tried 3 Multiple Logistic Regression algorithms: LDA, QDA and GBM. LDA has poor prediction capability(accuracy < 80%). QDA is better in accuracy(around 89%). GBM spent a very long time in training but didn't do a better job.
  - Then we tried Support Vector Machine with linear kernel. It also has poor prediction capability(accuracy < 80%), so we will not use it.
  - Finally, we tried Random Forest method. It works very well in prediction capability(accuracy > 99%), so we choose it as our final model.
      
```{r, cache=TRUE}
rfModel <- train(x=trainingX, y=trainingY, method = "rf", trControl = ctrl)
```

```{r}
rfModel
plot(rfModel)
```

### Out of Sample Error

According to the information of the model, in sample error is ```1 - 0.998 = 0.002```. 

Then we test with the testing set:
```{r, warning=FALSE}
rfPred <- predict(rfModel, testingX)
confusionMatrix(rfPred, testingY)
```

The accuracy is **0.9976**, so our out of sample error is ```1 - 0.9976 = 0.0024```.

### Testing
After building the model, we are ready to test.

First, we read the testing data. Then we still have to preprocess the data as we did in building the model.
```{r}
# start testing
pml_test <- read.csv("pml-testing.csv", stringsAsFactors = F)
data_test <- pml_test[,-(1:5)]

data_test$kurtosis_roll_belt <- as.numeric(data_test$kurtosis_roll_belt)
data_test$kurtosis_picth_belt <- as.numeric(data_test$kurtosis_picth_belt)
data_test$skewness_roll_belt <- as.numeric(data_test$skewness_roll_belt)
data_test$skewness_roll_belt.1 <- as.numeric(data_test$skewness_roll_belt.1)
data_test$max_yaw_belt <- as.numeric(data_test$max_yaw_belt)
data_test$min_yaw_belt <- as.numeric(data_test$min_yaw_belt)
data_test$kurtosis_roll_arm <- as.numeric(data_test$kurtosis_roll_arm)
data_test$kurtosis_picth_arm <- as.numeric(data_test$kurtosis_picth_arm)
data_test$kurtosis_yaw_arm <- as.numeric(data_test$kurtosis_yaw_arm)
data_test$skewness_roll_arm <- as.numeric(data_test$skewness_roll_arm)
data_test$skewness_pitch_arm <- as.numeric(data_test$skewness_pitch_arm)
data_test$skewness_yaw_arm <- as.numeric(data_test$skewness_yaw_arm)
data_test$kurtosis_roll_dumbbell <- as.numeric(data_test$kurtosis_roll_dumbbell)
data_test$kurtosis_picth_dumbbell <- as.numeric(data_test$kurtosis_picth_dumbbell)
data_test$skewness_roll_dumbbell <- as.numeric(data_test$skewness_roll_dumbbell)
data_test$skewness_pitch_dumbbell <- as.numeric(data_test$skewness_pitch_dumbbell)
data_test$max_yaw_dumbbell <- as.numeric(data_test$max_yaw_dumbbell)
data_test$min_yaw_dumbbell <- as.numeric(data_test$min_yaw_dumbbell)
data_test$kurtosis_roll_forearm <- as.numeric(data_test$kurtosis_roll_forearm)
data_test$kurtosis_picth_forearm <- as.numeric(data_test$kurtosis_picth_forearm)
data_test$skewness_roll_forearm <- as.numeric(data_test$skewness_roll_forearm)
data_test$skewness_pitch_forearm <- as.numeric(data_test$skewness_pitch_forearm)
data_test$max_yaw_forearm <- as.numeric(data_test$max_yaw_forearm)
data_test$min_yaw_forearm <- as.numeric(data_test$min_yaw_forearm)

# we only need predictors the model needed
df_test <- data_test[, colnames(df)]

# imput NA as 0
df_test[is.na(df_test)] <- 0
```

```{r}
# predict testing data
answers <- predict(rfModel, df_test)
```

